{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bhagyasharma/nlp_commonsenseqa/blob/master/csqa_test_random_input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6EDCRYYC0Xu"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "Upload bert_config.json & pytorch_model.bin files from the 'models' folder into your Google Drive. Once you've mounted your drive (in the 3rd cell of this notebook), set the file path of these 2 files in cell 4. This takes care of the model loading portion.  \n",
    "\n",
    "To test random input, fill out the Question/Answer form by inputting a common sense question with 3 answer options. Run the remaining cells of the notebook to get your answer prediction by this model. \n",
    "\n",
    "####Example of Common Sense Questions:  \n",
    "1. Why do people read gossip magazines? **A) entertained** B) get information C) improve know how  \n",
    "\n",
    "2. When a person admits his mistakes, what are they doing? **A) act responsibly** B) learn to swim C) feel relieved  \n",
    "\n",
    "3. Where can books be read? A) shelf **B) table** C) backpack  \n",
    "\n",
    "** examples from https://www.tau-nlp.org/commonsenseqa\n",
    "\n",
    "\n",
    "\n",
    "For more information about this model, see https://github.com/bhagyasharma/nlp_commonsenseqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kro-HAGTXWIr"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "H8-AOwBXvDip",
    "outputId": "1bd0fd6e-74bc-495f-ec37-84c541b2c2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 4.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.106)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.106 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.106)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (2.5.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (1.11.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.1\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IHjzjN7_vE8v",
    "outputId": "2ca1577d-a2a8-4a40-9e1b-9353ee3bc3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert.modeling import BertForMultipleChoice, BertConfig\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5inOLAkXbdT"
   },
   "source": [
    "#Import saved model from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Hp_EeB2ZJE9V",
    "outputId": "cd0e23d5-cc43-4e01-b735-1d4b988fa053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAa9b7aUJ6fZ"
   },
   "outputs": [],
   "source": [
    "''' Set file names here'''\n",
    "config_file = '/content/gdrive/My Drive/Colab Notebooks/bert_config.json'\n",
    "model_bin_file = '/content/gdrive/My Drive/Colab Notebooks/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9U6zxNE8cxa"
   },
   "source": [
    "# Load model and supporting methods/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q78SB9vxvOf2"
   },
   "outputs": [],
   "source": [
    "# Load a trained model and config that you have fine-tuned\n",
    "config = BertConfig(config_file)\n",
    "model = BertForMultipleChoice(config, num_choices=3)\n",
    "model.load_state_dict(torch.load(model_bin_file, map_location=torch.device('cpu')))\n",
    "\n",
    "class CSQAExample(object):\n",
    "    \"\"\"A single training/test example for the CSQA dataset.\"\"\"\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 choice_0,\n",
    "                 choice_1,\n",
    "                 choice_2,\n",
    "                 choice_3 = None,\n",
    "                 choice_4 = None,\n",
    "                 label = None):\n",
    "        self.question = question\n",
    "        self.choices = [\n",
    "            choice_0,\n",
    "            choice_1,\n",
    "            choice_2,\n",
    "            choice_3,\n",
    "            choice_4\n",
    "        ]\n",
    "        self.choices = list(filter(None.__ne__, self.choices))\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        l = [\n",
    "            \"question: {}\".format(self.question),\n",
    "            \"choice_0: {}\".format(self.choices[0]),\n",
    "            \"choice_1: {}\".format(self.choices[1]),\n",
    "            \"choice_2: {}\".format(self.choices[2]),\n",
    "            \"choice_3: {}\".format(self.choices[3]),\n",
    "            \"choice_4: {}\".format(self.choices[4])\n",
    "        ]\n",
    "\n",
    "        if self.label is not None:\n",
    "            l.append(\"label: {}\".format(self.label))\n",
    "\n",
    "        return \", \".join(l)\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,\n",
    "                 choices_features,\n",
    "                 label\n",
    "\n",
    "    ):\n",
    "        self.choices_features = [\n",
    "            {\n",
    "                'input_ids': input_ids,\n",
    "                'input_mask': input_mask,\n",
    "                'segment_ids': segment_ids\n",
    "            }\n",
    "            for _, input_ids, input_mask, segment_ids in choices_features\n",
    "        ]\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def read_csqa_example(question, ac0, ac1, ac2):\n",
    "    examples = []\n",
    "    examples.append(\n",
    "        CSQAExample(\n",
    "        question = question,\n",
    "\n",
    "        choice_0 = ac0,\n",
    "        choice_1 = ac1,\n",
    "        choice_2 = ac2))\n",
    "\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    # Use this formatting for tokenization\n",
    "    # Each choice will be tokenized accordingly:\n",
    "    # - [CLS] question [SEP] choice_1 [SEP]\n",
    "    # - [CLS] question [SEP] choice_2 [SEP]\n",
    "    # - [CLS] question [SEP] choice_3 [SEP]\n",
    "    # The model will output a single value for each input. To get the\n",
    "    # final decision of the model, we will run a softmax over these 3\n",
    "    # outputs.\n",
    "    features = []\n",
    "    for example_index, example in enumerate(examples):\n",
    "        context_tokens = tokenizer.tokenize(example.question)\n",
    "\n",
    "        choices_features = []\n",
    "        for choice_index, choice in enumerate(example.choices):\n",
    "            # We create a copy of the context tokens in order to be\n",
    "            # able to shrink it according to choice_tokens\n",
    "            context_tokens_choice = context_tokens[:]\n",
    "            choice_tokens = tokenizer.tokenize(choice)\n",
    "            # Modifies `context_tokens_choice` and `choice_tokens` in\n",
    "            # place so that the total length is less than the\n",
    "            # specified length.  Account for [CLS], [SEP], [SEP] with\n",
    "            # \"- 3\"\n",
    "            \n",
    "            _truncate_seq_pair(context_tokens_choice, choice_tokens, max_seq_length - 3)\n",
    "\n",
    "            tokens = [\"[CLS]\"] + context_tokens_choice + [\"[SEP]\"] + choice_tokens + [\"[SEP]\"]\n",
    "            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(choice_tokens) + 1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding = [0] * (max_seq_length - len(input_ids))\n",
    "            input_ids += padding\n",
    "            input_mask += padding\n",
    "            segment_ids += padding\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n",
    "\n",
    "        label = example.label\n",
    "        \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                choices_features = choices_features,\n",
    "                label = label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_length:\n",
    "        return\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "        tokens_a.pop()\n",
    "    else:\n",
    "        tokens_b.pop()\n",
    "\n",
    "def select_field(features, field):\n",
    "    return [\n",
    "        [\n",
    "            choice[field]\n",
    "            for choice in feature.choices_features\n",
    "        ]\n",
    "        for feature in features\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WEi_XnXXm2Y"
   },
   "source": [
    "#Add test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXcHZZM7vWwN"
   },
   "outputs": [],
   "source": [
    "question = \"Why do people read gossip magazines?\" #@param {type:\"string\"}\n",
    "answer_choice_0 = \"entertained\" #@param {type:\"string\"}\n",
    "answer_choice_1 = \"get information\" #@param {type:\"string\"}\n",
    "answer_choice_2 = \"improve know how\" #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nR4gBAuzXvO3"
   },
   "source": [
    "#Get predicted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nvNZZ6aNL3Sn",
    "outputId": "9cf0f99d-5827-46a4-f609-2f693b78288b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer is Choice 0: entertained\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "device = torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed_all(42)\n",
    "  \n",
    "model.to(device)\n",
    "if n_gpu > 1:\n",
    "  model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "eval_examples = read_csqa_example(question, answer_choice_0, answer_choice_1, answer_choice_2)\n",
    "eval_features = convert_examples_to_features(eval_examples, tokenizer, 128, False)\n",
    "all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
    "all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
    "all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "eval_accuracy = 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions = []\n",
    "for input_ids, input_mask, segment_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    outputs = np.argmax(logits, axis=1).tolist()\n",
    "    predictions.extend(outputs)\n",
    "\n",
    "answer_choices = [answer_choice_0, answer_choice_1, answer_choice_2]\n",
    "print('Answer is Choice ' + str(predictions[0]) + ': '+ answer_choices[predictions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cl-Do84aPXUR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "csqa_test_random_input.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
